{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Khaled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Khaled\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>image</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>style</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>verified</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks even better in person. Be careful to not...</td>\n",
       "      <td>08 4, 2014</td>\n",
       "      <td>A24E3SXTC62LJI</td>\n",
       "      <td>Claudia Valdivia</td>\n",
       "      <td>{'Color:': ' Bling'}</td>\n",
       "      <td>Can't stop won't stop looking at it</td>\n",
       "      <td>1407110400</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>When you don't want to spend a whole lot of ca...</td>\n",
       "      <td>02 12, 2014</td>\n",
       "      <td>A269FLZCB4GIPV</td>\n",
       "      <td>sarah ponce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1392163200</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>so the case came on time, i love the design. I...</td>\n",
       "      <td>02 8, 2014</td>\n",
       "      <td>AB6CHQWHZW4TV</td>\n",
       "      <td>Kai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Its okay</td>\n",
       "      <td>1391817600</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>DON'T CARE FOR IT.  GAVE IT AS A GIFT AND THEY...</td>\n",
       "      <td>02 4, 2014</td>\n",
       "      <td>A1M117A53LEI8</td>\n",
       "      <td>Sharon Williams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CASE</td>\n",
       "      <td>1391472000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7508492919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>I liked it because it was cute, but the studs ...</td>\n",
       "      <td>02 3, 2014</td>\n",
       "      <td>A272DUT8M88ZS8</td>\n",
       "      <td>Bella Rodriguez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cute!</td>\n",
       "      <td>1391385600</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin image  overall  \\\n",
       "0  7508492919   NaN        5   \n",
       "1  7508492919   NaN        5   \n",
       "2  7508492919   NaN        3   \n",
       "3  7508492919   NaN        2   \n",
       "4  7508492919   NaN        4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Looks even better in person. Be careful to not...   08 4, 2014   \n",
       "1  When you don't want to spend a whole lot of ca...  02 12, 2014   \n",
       "2  so the case came on time, i love the design. I...   02 8, 2014   \n",
       "3  DON'T CARE FOR IT.  GAVE IT AS A GIFT AND THEY...   02 4, 2014   \n",
       "4  I liked it because it was cute, but the studs ...   02 3, 2014   \n",
       "\n",
       "       reviewerID      reviewerName                 style  \\\n",
       "0  A24E3SXTC62LJI  Claudia Valdivia  {'Color:': ' Bling'}   \n",
       "1  A269FLZCB4GIPV       sarah ponce                   NaN   \n",
       "2   AB6CHQWHZW4TV               Kai                   NaN   \n",
       "3   A1M117A53LEI8   Sharon Williams                   NaN   \n",
       "4  A272DUT8M88ZS8   Bella Rodriguez                   NaN   \n",
       "\n",
       "                               summary  unixReviewTime  verified vote  \n",
       "0  Can't stop won't stop looking at it      1407110400      True  NaN  \n",
       "1                                    1      1392163200      True  NaN  \n",
       "2                             Its okay      1391817600      True  NaN  \n",
       "3                                 CASE      1391472000      True  NaN  \n",
       "4                                Cute!      1391385600      True  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"dataset/Cell_Phones_and_Accessories_5.json.gz\"\n",
    "df = pd.read_json(data, lines = True, compression = \"gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asin', 'image', 'overall', 'reviewText', 'reviewTime', 'reviewerID',\n",
       "       'reviewerName', 'style', 'summary', 'unixReviewTime', 'verified',\n",
       "       'vote'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reviews are verified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verified\n",
       "False    141113\n",
       "True     987324\n",
       "Name: unixReviewTime, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('verified')['unixReviewTime'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open point: how do we address it? (If we want to do it, of course :) )\n",
    "# Filtering the dataset\n",
    "The first operation we will perform is the removal of punctuation characters and lowercase all letters: these operation will be useful for reducing the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].str.replace('[.,;:;!?]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.reviewText[df.reviewText.notnull()].values\n",
    "y = df.overall[df.reviewText.notnull()].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127672,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127672,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the values split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 81506, 2: 57166, 3: 98214, 4: 184351, 5: 706435}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_value, counts = np.unique(y, return_counts=True)\n",
    "dict(zip(star_value, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.07227810923743784,\n",
       " 2: 0.05069381876999695,\n",
       " 3: 0.08709447427975511,\n",
       " 4: 0.16347927411516824,\n",
       " 5: 0.6264543235976419}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(star_value, counts/len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced dataset!\n",
    "We'd like to split the ratings as follows:\n",
    "- 1,2 and 3 will be considered negative\n",
    "- 4 and 5 will be considered positive\n",
    "\n",
    "The main reason why we'd like to proceed as follows is that, on Amazon, the most restrictive filter is the \"4 star +\" one and is used for filtering the returned results: a given vendor would like to have her/his products shown after this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2sent(n):\n",
    "    if n >= 4:\n",
    "        return \"positive\"\n",
    "    if n <= 3:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_is_positive = y > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 236886, True: 890786}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_categories, counts = np.unique(sentiment_is_positive, return_counts=True)\n",
    "dict(zip(sentiment_categories, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 0.2100664022871899, True: 0.7899335977128101}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(sentiment_categories, counts/len(sentiment_is_positive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiment_is_positive, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this case is nice well-made and fits snugly on the phone  i purchased this case along with another brand that was 100% tpu material  while i can see that this case is very scratch resistant i think i prefer a case that is 100% tpu because of improved grip and non-slip properties  the hard polycarbonate material on the back of this case will let the phone slide on a smooth surface like a table  i just prefer a case that has more of a \"grippy\" back so that the phone doesn\\'t slide on a smooth surface  but that\\'s just my preference'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = [word_tokenize(sentence) for sentence in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized = [word_tokenize(sentence) for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump(X_train_tokenized, 'X_train_tokenized.joblib')\n",
    "#dump(X_test_tokenized, 'X_test_tokenized.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopws = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words, however, have an important meaning for our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stopws[-36:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopws = stopws[:-36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other words with a useful meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2save = [\"but\", \"while\", \"against\", \"not\", \"only\", \"very\", 'don', \"don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words2save:\n",
    "    stopws.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stop = []\n",
    "for sentence in X_train_tokenized:\n",
    "    filtered_sentence = [word for word in sentence if word not in stopws]\n",
    "    X_train_tokenized_stop.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_stop = []\n",
    "for sentence in X_test_tokenized:\n",
    "    filtered_sentence = [word for word in sentence if word not in stopws]\n",
    "    X_test_tokenized_stop.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_stop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Because it is necessary to install Visual C++ before installing the package 'pyStemmer' via pip https://support.microsoft.com/it-it/help/2977003/the-latest-supported-visual-c-downloads I will use nltk library even though it is less efficient\n",
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stemmed_ps = []\n",
    "for sentence in X_train_tokenized:\n",
    "    X_train_tokenized_stemmed_ps.append([ps.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stemmed_ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_stemmed_ps = []\n",
    "for sentence in X_test_tokenized:\n",
    "    X_test_tokenized_stemmed_ps.append([ps.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stemmed_ls = []\n",
    "for sentence in X_train_tokenized:\n",
    "    X_train_tokenized_stemmed_ls.append([ls_stemmer.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_stemmed_ls = []\n",
    "for sentence in X_test_tokenized:\n",
    "    X_test_tokenized_stemmed_ls.append([ls_stemmer.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "For computing TF-IDF matrix we need to rebuild the sentences. Let's do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_ps_sent = []\n",
    "for sentence in X_train_tokenized_stemmed_ps:\n",
    "    X_train_tokenized_ps_sent.append(\" \".join(sentence))\n",
    "X_train_tokenized_ps_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_ps_sent = []\n",
    "for sentence in X_test_tokenized_stemmed_ps:\n",
    "    X_test_tokenized_ps_sent.append(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_ls_sent = []\n",
    "for sentence in X_train_tokenized_stemmed_ls:\n",
    "    X_train_tokenized_ls_sent.append(\" \".join(sentence))\n",
    "X_train_tokenized_ls_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_ls_sent = []\n",
    "for sentence in X_test_tokenized_stemmed_ls:\n",
    "    X_test_tokenized_ls_sent.append(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute TF-IDF matrix\n",
    "For avoiding a high number of features I will set the following two constrainst:\n",
    "- A term should have a frequency >= 5 in the entire corpus\n",
    "- The best 50 000 features are kept\n",
    "\n",
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect_ps = TfidfVectorizer(min_df= 5, max_features = 50000)\n",
    "X_train_tfidf_ps = tfidf_vect_ps.fit_transform(X_train_tokenized_ps_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tfidf_vect_ps, 'tfidf_vect_ps.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_ps = tfidf_vect_ps.transform(X_test_tokenized_ps_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vect_ps.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect_ls = TfidfVectorizer(min_df= 5, max_features = 50000)\n",
    "X_train_tfidf_ls = tfidf_vect_ls.fit_transform(X_train_tokenized_ls_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tfidf_vect_ls, 'tfidf_vect_ls.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_ls = tfidf_vect_ls.transform(X_test_tokenized_ls_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vect_ls.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store stemmed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(X_train_tfidf_ps, 'X_train_tfidf_ps.joblib')\n",
    "dump(X_test_tfidf_ps, 'X_test_tfidf_ps.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(X_train_tfidf_ls, 'X_train_tfidf_ls.joblib')\n",
    "dump(X_test_tfidf_ls, 'X_test_tfidf_ls.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "## Multinomial Naive-Bayes\n",
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf_ps, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, f1_score, fbeta_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = clf.score(X_train_tfidf_ps, y_train) # Train Accuracy\n",
    "test_score = clf.score(X_test_tfidf_ps, y_test)    # Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_tfidf_ps)\n",
    "prec = precision_score(y_test, predictions) # Precision\n",
    "rec = recall_score(y_test, predictions) # Recall\n",
    "f1 = f1_score(y_test, predictions) # F1\n",
    "f2 = fbeta_score(y_test, predictions, 2) # F2\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = clf.predict_proba(X_test_tfidf_ps)\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_strings = [\"Train Accuracy\", \"Test Accuracy\", \"Test Precision\",\n",
    "                  \"Test Recall\", \"F1\", \"F2\", \"P/R AUC\"]\n",
    "scores = [train_score, test_score, prec, rec, f1, f2, auc_score]\n",
    "print((\"{:20s} {:.5f}\\n\"*7)[:-1].format(*itertools.chain(*zip(scores_strings, scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve: AUC=%0.2f' % auc_score)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf_ls, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = clf.score(X_train_tfidf_ls, y_train) # Train Accuracy\n",
    "test_score = clf.score(X_test_tfidf_ls, y_test)    # Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_tfidf_ls)\n",
    "prec = precision_score(y_test, predictions) # Precision\n",
    "rec = recall_score(y_test, predictions) # Recall\n",
    "f1 = f1_score(y_test, predictions) # F1\n",
    "f2 = fbeta_score(y_test, predictions, 2) # F2\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "proba = clf.predict_proba(X_test_tfidf_ls)\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_strings = [\"Train Accuracy\", \"Test Accuracy\", \"Test Precision\",\n",
    "                  \"Test Recall\", \"F1\", \"F2\", \"P/R AUC\"]\n",
    "scores = [train_score, test_score, prec, rec, f1, f2, auc_score]\n",
    "print((\"{:20s} {:.5f}\\n\"*7)[:-1].format(*itertools.chain(*zip(scores_strings, scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve: AUC=%0.2f' % auc_score)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose=2)\n",
    "clf.fit(X_train_tfidf_ps, y_train) # it takes around 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = clf.score(X_train_tfidf_ps, y_train) # Train Accuracy\n",
    "test_score = clf.score(X_test_tfidf_ps, y_test)    # Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_tfidf_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are more encouraging! The problem is that it's way slower than Multinomial NB.\n",
    "## TruncatedSVD\n",
    "The X_train vector has around 20k features: for speeding up the training phase it may be good to use dimensionality reduction methods. Their goal is to preserve \"expressive power\" while reducing dataset dimensionality.\n",
    "Because the TFIDF matrix is a sparse one, one of the best method for performing dimensionality reduction is \"TruncatedSVD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsvd = TruncatedSVD(n_components=500, random_state=42)\n",
    "X_train_tfidf_ps_svd = tsvd.fit_transform(X_train_tfidf_ps)\n",
    "X_test_tfidf_ps_svd = tsvd.transform(X_test_tfidf_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train with old features: \",np.array(X_train_tfidf_ps).shape)\n",
    "print(\"train with new features:\" ,np.array(X_train_tfidf_ps_svd).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store SVD-transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(X_train_tfidf_ps_svd, 'X_train_tfidf_ps_svd.joblib')\n",
    "dump(X_test_tfidf_ps_svd, 'X_test_tfidf_ps_svd.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "Multinomial NB won't be used for the following reasons: https://stackoverflow.com/questions/24169238/dealing-with-negative-values-in-sklearn-multinomialnb\n",
    "#### Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose=2)\n",
    "clf.fit(X_train_tfidf_ps_svd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_tfidf_ps_svd)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sentences without stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_stemmed_ps_stop = []\n",
    "for sentence in X_train_tokenized_stop:\n",
    "    X_train_tokenized_stemmed_ps_stop.append([ps.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_stemmed_ps_stop = []\n",
    "for sentence in X_test_tokenized_stop:\n",
    "    X_test_tokenized_stemmed_ps_stop.append([ps.stem(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized_ps_sent_stop = []\n",
    "for sentence in X_train_tokenized_stemmed_ps_stop:\n",
    "    X_train_tokenized_ps_sent_stop.append(\" \".join(sentence))\n",
    "X_train_tokenized_ps_sent_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized_ps_sent_stop = []\n",
    "for sentence in X_test_tokenized_stemmed_ps_stop:\n",
    "    X_test_tokenized_ps_sent_stop.append(\" \".join(sentence))\n",
    "X_test_tokenized_ps_sent_stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect_ps_stop = TfidfVectorizer(min_df= 5, max_features = 50000)\n",
    "X_train_tfidf_ps_stop = tfidf_vect_ps_stop.fit_transform(X_train_tokenized_ps_sent_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_ps_stop = tfidf_vect_ps_stop.transform(X_test_tokenized_ps_sent_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf_ps_stop, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_ps_stop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_ps_stop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_tfidf_ps_stop)\n",
    "prec = precision_score(y_test, predictions) # Precision\n",
    "rec = recall_score(y_test, predictions) # Recall\n",
    "f1 = f1_score(y_test, predictions) # F1\n",
    "f2 = fbeta_score(y_test, predictions, 2) # F2\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "proba = clf.predict_proba(X_test_tfidf_ps_stop)\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = auc(recall, precision)\n",
    "scores_strings = [\"Train Accuracy\", \"Test Accuracy\", \"Test Precision\",\n",
    "                  \"Test Recall\", \"F1\", \"F2\", \"P/R AUC\"]\n",
    "scores = [train_score, test_score, prec, rec, f1, f2, auc_score]\n",
    "print((\"{:20s} {:.5f}\\n\"*7)[:-1].format(*itertools.chain(*zip(scores_strings, scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve: AUC=%0.2f' % auc_score)\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
