{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from sklearn.metrics import precision_recall_curve, auc,confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: english_stemmer.stemWords(analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "english_stemmer = Stemmer.Stemmer('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>mrsshinde</td>\n",
       "      <td>o</td>\n",
       "      <td>RT @mrsshinde: @SamsungMobile @Moto @oneplus @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>olutobi_og</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile kindly include play next in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>themobileindian</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile has started rolling out the And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>sobakhani</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile how to find lost Samsung note 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>Imchetan_p</td>\n",
       "      <td>n</td>\n",
       "      <td>@SamsungMobile @SamsungIndia @Samsung I must s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang user_screen_name sent  \\\n",
       "0   en        mrsshinde    o   \n",
       "1   en       olutobi_og    o   \n",
       "2   en  themobileindian    o   \n",
       "3   en        sobakhani    o   \n",
       "4   en       Imchetan_p    n   \n",
       "\n",
       "                                                text  \n",
       "0  RT @mrsshinde: @SamsungMobile @Moto @oneplus @...  \n",
       "1  @SamsungMobile kindly include play next in the...  \n",
       "2  @SamsungMobile has started rolling out the And...  \n",
       "3  @SamsungMobile how to find lost Samsung note 1...  \n",
       "4  @SamsungMobile @SamsungIndia @Samsung I must s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_tweets = pd.read_csv(\"dataset/SamsungTweetsSent.csv\")\n",
    "sam_tweets[\"text\"] = sam_tweets.full_text\n",
    "sam_tweets.drop(\"full_text\", axis = 1, inplace = True)\n",
    "sam_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>@Razer debuted an incredibly compact all-in-on...</td>\n",
       "      <td>techthelead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>In keynote address, @Delta Unveils New #OOH Pa...</td>\n",
       "      <td>YourOAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>üëç We are ready for Day 2 at #CES2020. Discover...</td>\n",
       "      <td>Sio_db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>GO-&amp;gt; #CES2020 #France #USA !\\n#BusinessFran...</td>\n",
       "      <td>dillardmarg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>The industry's smallest and lightest 4K60P pro...</td>\n",
       "      <td>HoldanBlog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               text     username\n",
       "0   en  @Razer debuted an incredibly compact all-in-on...  techthelead\n",
       "1   en  In keynote address, @Delta Unveils New #OOH Pa...     YourOAAA\n",
       "2   en  üëç We are ready for Day 2 at #CES2020. Discover...       Sio_db\n",
       "3   en  GO-&gt; #CES2020 #France #USA !\\n#BusinessFran...  dillardmarg\n",
       "4   en  The industry's smallest and lightest 4K60P pro...   HoldanBlog"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ces_tweets = pd.read_csv(\"dataset/ces2020_tweets_full_text.csv\")\n",
    "ces_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries and preprocessing functions from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_repl = {\n",
    "    # positive emoticons\n",
    "    r\":-?d+\": \" good \",  r\":[- ]?\\)+\": \" good \", r\";-?\\)+\": \" good \",\n",
    "    r\"\\(+-?:\": \" good \", r\"=\\)+\" : \" good \", r\"<3\" : \" good \",\n",
    "    # negative emoticons\n",
    "    r\"[\\s\\r\\t\\n]+:/+\": \" bad \", r\":\\\\+\": \" bad \", r\"[\\s\\r\\t\\n]+\\)-?:\": \" bad \",\n",
    "    r\":-?\\(+\": \" bad \", r\"[\\s\\t\\r\\n]+d+-?:\": \" bad \"\n",
    "}\n",
    "\n",
    "contracted_repl = {\n",
    "    # casi particolari\n",
    "    r\"won\\'t\" : \"will not\", r\"won\\'\" : \"will not\", r\"can\\'t\": \"can not\", r\"shan\\'t\": \"shall not\",\n",
    "    r\"shan\\'\": \"shall not\", r\"ain\\'t\": \"is not\", r\"ain\\'\": \"is not\",\n",
    "    # casi generali\n",
    "    r\"n\\'t\": \" not\", r\"\\'t\": \" not\", r\"n\\'\": \" not\", r\"\\'s\": \" is\", r\"\\'ve\": \" have\", \n",
    "    r\"\\'re\": \" are\", \n",
    "    r\"\\'ll\": \" will\", r\"\\'d\": \" would\",\n",
    "}\n",
    "\n",
    "with open('slang_subset_manual.json', 'r') as fid:\n",
    "    slang_repl = json.load(fid)\n",
    "    \n",
    "def preprocess(sent, translate_slang = True):\n",
    "    \n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'^<div id=\"video.*>&nbsp;', '', sent) # Video-review part\n",
    "    sent = re.sub('https?://[A-Za-z0-9./]+', '', sent) # URLs\n",
    "    \n",
    "    for k in emoticon_repl:\n",
    "        sent = re.sub(k, emoticon_repl[k], sent)\n",
    "\n",
    "    if translate_slang:\n",
    "        for k in slang_repl:\n",
    "            sent = re.sub(r\"\\b\"+re.escape(k)+r\"\\b\", slang_repl[k], sent)\n",
    "        \n",
    "    for k in contracted_repl:\n",
    "        sent = re.sub(k, contracted_repl[k], sent)\n",
    "    \n",
    "    sent = re.sub('[/]+', ' ', sent) # word1/word2 to word1 word2\n",
    "    sent = re.sub('[^A-Za-z0-9-_ ]+', '', sent)\n",
    "    sent = re.sub('\\b\\d+\\b', '', sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shold we remove Hashtags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets_df):\n",
    "    tweets_df[\"textPreprocessed\"] = tweets_df.text\n",
    "    tweets_df[\"textPreprocessed\"] =  tweets_df[\"textPreprocessed\"].str.replace(\"@\\w+\", \"\") # remove AT's\n",
    "    tweets_df[\"textPreprocessed\"] = tweets_df[\"textPreprocessed\"].str.replace(\"^(RT)+\", \"\") # Remove RT at beginning of retweets\n",
    "    \n",
    "    # Add stuff probably\n",
    "    \n",
    "    tweets_df[\"textPreprocessed\"] = tweets_df[\"textPreprocessed\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>textPreprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>@Razer debuted an incredibly compact all-in-on...</td>\n",
       "      <td>techthelead</td>\n",
       "      <td>debuted an incredibly compact all-in-one syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>In keynote address, @Delta Unveils New #OOH Pa...</td>\n",
       "      <td>YourOAAA</td>\n",
       "      <td>in keynote address  unveils new ooh parallel r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>üëç We are ready for Day 2 at #CES2020. Discover...</td>\n",
       "      <td>Sio_db</td>\n",
       "      <td>we are ready for day 2 at ces2020 discover ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>GO-&amp;gt; #CES2020 #France #USA !\\n#BusinessFran...</td>\n",
       "      <td>dillardmarg</td>\n",
       "      <td>go-gt ces2020 france usa businessfrance cce in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>The industry's smallest and lightest 4K60P pro...</td>\n",
       "      <td>HoldanBlog</td>\n",
       "      <td>the industry is smallest and lightest 4k60p pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               text     username  \\\n",
       "0   en  @Razer debuted an incredibly compact all-in-on...  techthelead   \n",
       "1   en  In keynote address, @Delta Unveils New #OOH Pa...     YourOAAA   \n",
       "2   en  üëç We are ready for Day 2 at #CES2020. Discover...       Sio_db   \n",
       "3   en  GO-&gt; #CES2020 #France #USA !\\n#BusinessFran...  dillardmarg   \n",
       "4   en  The industry's smallest and lightest 4K60P pro...   HoldanBlog   \n",
       "\n",
       "                                    textPreprocessed  \n",
       "0   debuted an incredibly compact all-in-one syst...  \n",
       "1  in keynote address  unveils new ooh parallel r...  \n",
       "2   we are ready for day 2 at ces2020 discover ou...  \n",
       "3  go-gt ces2020 france usa businessfrance cce in...  \n",
       "4  the industry is smallest and lightest 4k60p pr...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweets(ces_tweets)\n",
    "ces_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "      <th>textPreprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>mrsshinde</td>\n",
       "      <td>o</td>\n",
       "      <td>RT @mrsshinde: @SamsungMobile @Moto @oneplus @...</td>\n",
       "      <td>we must work to save safeguard human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>olutobi_og</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile kindly include play next in the...</td>\n",
       "      <td>kindly include play next in the next samsungm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>themobileindian</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile has started rolling out the And...</td>\n",
       "      <td>has started rolling out the android 10 update...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>sobakhani</td>\n",
       "      <td>o</td>\n",
       "      <td>@SamsungMobile how to find lost Samsung note 1...</td>\n",
       "      <td>how to find lost samsung note 10 plus in paki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>Imchetan_p</td>\n",
       "      <td>n</td>\n",
       "      <td>@SamsungMobile @SamsungIndia @Samsung I must s...</td>\n",
       "      <td>i must say that your sales services really ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang user_screen_name sent  \\\n",
       "0   en        mrsshinde    o   \n",
       "1   en       olutobi_og    o   \n",
       "2   en  themobileindian    o   \n",
       "3   en        sobakhani    o   \n",
       "4   en       Imchetan_p    n   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @mrsshinde: @SamsungMobile @Moto @oneplus @...   \n",
       "1  @SamsungMobile kindly include play next in the...   \n",
       "2  @SamsungMobile has started rolling out the And...   \n",
       "3  @SamsungMobile how to find lost Samsung note 1...   \n",
       "4  @SamsungMobile @SamsungIndia @Samsung I must s...   \n",
       "\n",
       "                                    textPreprocessed  \n",
       "0            we must work to save safeguard human...  \n",
       "1   kindly include play next in the next samsungm...  \n",
       "2   has started rolling out the android 10 update...  \n",
       "3   how to find lost samsung note 10 plus in paki...  \n",
       "4     i must say that your sales services really ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweets(sam_tweets)\n",
    "sam_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiwordnet Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\gianc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\gianc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns, for a list of tweets:\n",
    "\n",
    "- Their tokens and their tags\n",
    "- Their positivity score\n",
    "- Their negativity score\n",
    "- Their sentiment score (-1 for negative, 0 for neutral, 1 for positive)\n",
    "\n",
    "TODO: add reference to article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_senti(X):\n",
    "    lem = WordNetLemmatizer()\n",
    "    pstem = PorterStemmer()\n",
    "    X_tagged = []\n",
    "    li_swn=[]\n",
    "    li_swn_pos=[]\n",
    "    li_swn_neg=[]\n",
    "    missing_words=[]\n",
    "    for i in range(len(X)):\n",
    "        text = X[i]\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tagged_sent = nltk.tag.pos_tag(tokens)\n",
    "        store_it = [(word, nltk.tag.map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
    "        X_tagged.append(store_it)\n",
    "        #print(\"Tagged Parts of Speech:\",store_it)\n",
    "\n",
    "        pos_total=0\n",
    "        neg_total=0\n",
    "        for word,tag in store_it:\n",
    "            # print(tag)\n",
    "            if(tag=='NOUN'):\n",
    "                tag='n'\n",
    "            elif(tag=='VERB'):\n",
    "                tag='v'\n",
    "            elif(tag=='ADJ'):\n",
    "                tag='a'\n",
    "            elif(tag=='ADV'):\n",
    "                tag = 'r'\n",
    "            else:\n",
    "                tag='nothing'\n",
    "\n",
    "                \n",
    "            if(tag!='nothing'):\n",
    "                concat = word+'.'+tag+'.01'\n",
    "                try:\n",
    "                    this_word_pos=swn.senti_synset(concat).pos_score()\n",
    "                    this_word_neg=swn.senti_synset(concat).neg_score()\n",
    "                    # print(word,tag,':',this_word_pos,this_word_neg)\n",
    "                except Exception as e:\n",
    "                    wor = lem.lemmatize(word)\n",
    "                    concat = wor+'.'+tag+'.01'\n",
    "                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
    "                    try:\n",
    "                        this_word_pos=swn.senti_synset(concat).pos_score()\n",
    "                        this_word_neg=swn.senti_synset(concat).neg_score()\n",
    "                    except Exception as e:\n",
    "                        wor = pstem.stem(word)\n",
    "                        concat = wor+'.'+tag+'.01'\n",
    "                        # Checking if there's a possiblity of lemmatized word be accepted\n",
    "                        try:\n",
    "                            this_word_pos=swn.senti_synset(concat).pos_score()\n",
    "                            this_word_neg=swn.senti_synset(concat).neg_score()\n",
    "                        except:\n",
    "                            missing_words.append(word)\n",
    "                            continue\n",
    "                pos_total+=this_word_pos\n",
    "                neg_total+=this_word_neg\n",
    "        li_swn_pos.append(pos_total)\n",
    "        li_swn_neg.append(neg_total)\n",
    "\n",
    "        if(pos_total!=0 or neg_total!=0):\n",
    "            if(pos_total>neg_total):\n",
    "                li_swn.append(1)\n",
    "            else:\n",
    "                li_swn.append(-1)\n",
    "        else:\n",
    "            li_swn.append(0)\n",
    "    # df_copy.insert(5,\"pos_score\",li_swn_pos,True)\n",
    "    # df_copy.insert(6,\"neg_score\",li_swn_neg,True)\n",
    "    # df_copy.insert(7,\"sent_score\",li_swn,True)\n",
    "    return X_tagged, li_swn_pos, li_swn_neg, li_swn\n",
    "    # end-of pos-tagging&sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(predictions, y_test):\n",
    "\n",
    "    prec = precision_score(y_test, predictions) # Precision\n",
    "    rec = recall_score(y_test, predictions) # Recall\n",
    "    f1 = f1_score(y_test, predictions) # F1\n",
    "    f2 = fbeta_score(y_test, predictions, 2) # F2\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    scores_strings = [\"Test Precision\",\n",
    "                      \"Test Recall\", \"F1\", \"F2\"]\n",
    "    \n",
    "    scores = [prec, rec, f1, f2]\n",
    "    \n",
    "    print((\"{:20s} {:.5f}\\n\"*4)[:-1].format(*itertools.chain(*zip(scores_strings, scores))))\n",
    "    \n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CES Tweets sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ces_X = np.array(ces_tweets.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ces_X_prep = np.array(ces_tweets.textPreprocessed.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üëç We are ready for Day 2 at #CES2020. Discover our innovative Database Siodb üë®\\u200düíª. Nicolas Penot and Gr√©gory Steulet will be pleased to show you it ensures data security and privacy. üëâhttps://t.co/dYRiXeHjyV\\n#SwissPavilion #SwissTech https://t.co/IBIqH8Qei9'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ces_X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we are ready for day 2 at ces2020 discover our innovative database siodb  nicolas penot and grgory steulet will be pleased to show you it ensures data security and privacy swisspavilion swisstech '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ces_X_prep[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ces_X_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ces_X_tagged, ces_pos_score_swn, ces_neg_score_swn, ces_sent_score_swn = pos_senti(ces_X_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWordNet classifies 463 tweets as negative, 319 as neutral and 1218 as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([ 463,  319, 1218], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ces_sent_score_swn, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load('tfidf_vect_pystemmer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('clf_nb_pystemmer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only the tweets labelled negative/positive by SentiWordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_ind = np.array(ces_sent_score_swn) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_ces_X = ces_X_prep[neg_pos_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_pos_ces_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_ces_score = np.array(ces_sent_score_swn)[neg_pos_ind] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ces_X_vect = vectorizer.transform(neg_pos_ces_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1681x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18265 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ces_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(ces_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([729, 952], dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[263, 200],\n",
       "       [466, 752]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(neg_pos_ces_score, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the binary scores from SentiWordNet are reliable, score our classifier (trained on the balanced dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision       0.78992\n",
      "Test Recall          0.61741\n",
      "F1                   0.69309\n",
      "F2                   0.64560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.36      0.57      0.44       463\n",
      "        True       0.79      0.62      0.69      1218\n",
      "\n",
      "    accuracy                           0.60      1681\n",
      "   macro avg       0.58      0.59      0.57      1681\n",
      "weighted avg       0.67      0.60      0.62      1681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores(predictions, neg_pos_ces_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samsung Tweets Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NA's from samsung tweets (one sentiment is missing, TODO relabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_tweets = sam_tweets[sam_tweets.sent.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take only tweets labelled as positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_tweets_sentiment = sam_tweets[sam_tweets.sent != \"o\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_X = np.array(sam_tweets_sentiment.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual labels: 139 negative, 19 positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([139,  18], dtype=int64))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_manual_y = np.array(sam_tweets_sentiment.sent)== \"p\"\n",
    "np.unique(sam_manual_y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_X_prep = np.array(sam_tweets_sentiment.textPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@SamsungMobile @SamsungIndia @Samsung I must say that your sales services really suck. I have been trying to reach out to your customer care via your samsung shop tollfree/email &amp; Live chat but none worked. Wasted an hour and really disappointed to place my trust in Samsung again'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   i must say that your sales services really suck i have been trying to reach out to your customer care via your samsung shop tollfree email amp live chat but none worked wasted an hour and really disappointed to place my trust in samsung again'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_X_prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sam_X_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_X_tagged, sam_pos_score_swn, sam_neg_score_swn, sam_sent_score_swn = pos_senti(sam_X_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWordNet classifies 98 tweets as negative, 6 as neutral and 53 as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([98,  6, 53], dtype=int64))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sam_sent_score_swn, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider negative and neutral tweets as negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_sent_score_swn_bin = np.array(sam_sent_score_swn) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([139,  18], dtype=int64))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sam_manual_y, return_counts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([104,  53], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sam_sent_score_swn_bin, return_counts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98, 41],\n",
       "       [ 6, 12]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(sam_manual_y, sam_sent_score_swn_bin)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load('tfidf_vect_pystemmer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('clf_nb_pystemmer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_X_vect = vectorizer.transform(sam_X_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<157x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(sam_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([136,  21], dtype=int64))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier against SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98,  6],\n",
       "       [38, 15]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(sam_sent_score_swn_bin, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision       0.71429\n",
      "Test Recall          0.28302\n",
      "F1                   0.40541\n",
      "F2                   0.32189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.72      0.94      0.82       104\n",
      "        True       0.71      0.28      0.41        53\n",
      "\n",
      "    accuracy                           0.72       157\n",
      "   macro avg       0.72      0.61      0.61       157\n",
      "weighted avg       0.72      0.72      0.68       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores(predictions, sam_sent_score_swn_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier against the manual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[132,   7],\n",
       "       [  4,  14]], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(sam_manual_y, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision       0.66667\n",
      "Test Recall          0.77778\n",
      "F1                   0.71795\n",
      "F2                   0.75269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.95      0.96       139\n",
      "        True       0.67      0.78      0.72        18\n",
      "\n",
      "    accuracy                           0.93       157\n",
      "   macro avg       0.82      0.86      0.84       157\n",
      "weighted avg       0.94      0.93      0.93       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores(predictions, sam_manual_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
